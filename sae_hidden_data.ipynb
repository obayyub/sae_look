{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119848e70>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:00<00:00, 72696.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#generate data for training as hidden state activations via LRH - sum of sparse overcomplete feature directions\n",
    "\n",
    "def generate_hidden_data(dim = 128, n_features = 512, \n",
    "                         n_samples = (2**10), sparsity = 10):\n",
    "    #basically want features Y times random vector w where w is sparse, then sum resulting vectors for hidden state\n",
    "    #overcomplete feature basis?\n",
    "    features = np.random.randn(n_features, dim)\n",
    "    features = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
    "\n",
    "    #init sparsity weights\n",
    "    weights = np.zeros((n_samples, n_features))\n",
    "    #generate sparsity weights\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        active_feats = np.random.choice(n_features, size=sparsity, replace=False)\n",
    "        weights[i, active_feats] = np.random.randn(sparsity)\n",
    "    #make hidden data via sum of sparse features\n",
    "    hidden_data = weights @ features\n",
    "\n",
    "    return torch.tensor(hidden_data, dtype=torch.float32)\n",
    "\n",
    "print(generate_hidden_data().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, input_dim, width_ratio=4, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.sae_hidden = input_dim * width_ratio\n",
    "        self.W_in = nn.Parameter(\n",
    "            nn.init.kaiming_uniform_(\n",
    "                torch.empty(input_dim, self.sae_hidden), nonlinearity=\"relu\"\n",
    "            )\n",
    "        )\n",
    "        self.b_in = nn.Parameter(torch.zeros(self.sae_hidden))\n",
    "        self.W_out = nn.Parameter(\n",
    "            nn.init.kaiming_uniform_(\n",
    "                torch.empty(self.sae_hidden, input_dim), nonlinearity=\"relu\"\n",
    "            )\n",
    "        )\n",
    "        self.b_out = nn.Parameter(torch.zeros(input_dim))\n",
    "        self.nonlinearity = activation\n",
    "\n",
    "    def _normalize_weights(self):\n",
    "        with torch.no_grad():\n",
    "            norms = self.W_out.norm(p=2, dim=0, keepdim=True)\n",
    "            self.W_out.div_(norms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - self.b_out\n",
    "        acts = self.nonlinearity(x @ self.W_in + self.b_in)\n",
    "        l1_regularization = acts.abs().sum()\n",
    "        l0 = (acts > 0).sum(dim=1).float().mean()\n",
    "        self._normalize_weights()\n",
    "\n",
    "        return l0, l1_regularization, acts@self.W_out + self.b_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, test_data, batch_size=128, n_epochs=1000, l1_lam=5e-5, weight_decay=1e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "    mse_criterion = nn.MSELoss()\n",
    "\n",
    "    n_batches = len(train_data) // batch_size\n",
    "    n_test_batches = len(test_data) // batch_size\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_test_loss = 0\n",
    "        total_mse_loss = 0\n",
    "        total_l1_loss = 0\n",
    "        total_l0 = 0\n",
    "        batch_perm = torch.randperm(len(train_data))\n",
    "        test_batch_perm = torch.randperm(len(test_data))\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            # Training\n",
    "            idx = batch_perm[i*batch_size: (i+1)*batch_size]\n",
    "            batch = train_data[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            l0, l1, recon_hiddens = model(batch)\n",
    "\n",
    "            recon_loss = mse_criterion(recon_hiddens, batch)\n",
    "            sparsity_loss = l1_lam * l1\n",
    "            loss = recon_loss + sparsity_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_l1_loss += sparsity_loss.item()\n",
    "            total_mse_loss += recon_loss.item()\n",
    "            total_l0 += l0\n",
    "\n",
    "            # Testing\n",
    "            if i < n_test_batches:\n",
    "                test_idx = test_batch_perm[i*batch_size: (i+1)*batch_size]\n",
    "                test_batch = test_data[test_idx]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    _, _, test_recon = model(test_batch)\n",
    "                    test_loss = mse_criterion(test_recon, test_batch)\n",
    "                    total_test_loss += test_loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = total_loss / n_batches\n",
    "            avg_test_loss = total_test_loss / n_test_batches\n",
    "            avg_l1_loss = total_l1_loss / n_batches\n",
    "            avg_l0 = total_l0 / n_batches\n",
    "            \n",
    "            print(f'Epoch {epoch}, Loss: {avg_loss:.4f}, '\n",
    "                  f'Test Loss: {avg_test_loss:.4f}, '\n",
    "                  f'L1: {avg_l1_loss:.4f}, '\n",
    "                  f'L0: {avg_l0:.4f}')\n",
    "\n",
    "    return {\n",
    "        'mse': total_mse_loss/n_batches,\n",
    "        'L0': total_l0/n_batches\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    sparsity = 20\n",
    "    hidden_dim = 128\n",
    "    width_factor = 4\n",
    "\n",
    "    data = generate_hidden_data(dim=hidden_dim, sparsity=sparsity)\n",
    "    train_size = int(0.8 * len(data))\n",
    "    train_data, test_data = data[:train_size], data[train_size:]\n",
    "    \n",
    "    relu_model = SAE(hidden_dim, width_factor, nn.ReLU())\n",
    "    print(\"Training ReLU model...\")\n",
    "    result = train(relu_model, train_data, test_data)\n",
    "            \n",
    "\n",
    "def run_DOE():\n",
    "    sparsities = [5, 10, 20, 30, 40, 50]\n",
    "    results = defaultdict(list)\n",
    "    hidden_dim = 128\n",
    "    width_factor = 4\n",
    "\n",
    "    for sparsity in sparsities:\n",
    "        for trial in range(10):\n",
    "            data = generate_hidden_data(dim=hidden_dim, sparsity=sparsity)\n",
    "            train_size = int(0.8 * len(data))\n",
    "            train_data, test_data = data[:train_size], data[train_size:]\n",
    "            \n",
    "            relu_model = SAE(hidden_dim, width_factor, nn.ReLU())\n",
    "            print(\"Training ReLU model...\")\n",
    "            result = train(relu_model, train_data)\n",
    "            results[sparsity].append(result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:00<00:00, 72970.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ReLU model...\n",
      "Epoch 0, Loss: 0.5255, Test Loss: 0.1853, L1: 0.3480, L0: 253.4700\n",
      "Epoch 10, Loss: 0.1833, Test Loss: 0.1245, L1: 0.0559, L0: 83.3477\n",
      "Epoch 20, Loss: 0.1593, Test Loss: 0.1372, L1: 0.0163, L0: 28.8008\n",
      "Epoch 30, Loss: 0.1508, Test Loss: 0.1365, L1: 0.0212, L0: 32.2318\n",
      "Epoch 40, Loss: 0.1416, Test Loss: 0.1304, L1: 0.0248, L0: 31.0703\n",
      "Epoch 50, Loss: 0.1351, Test Loss: 0.1225, L1: 0.0283, L0: 30.0286\n",
      "Epoch 60, Loss: 0.1299, Test Loss: 0.1183, L1: 0.0311, L0: 28.9271\n",
      "Epoch 70, Loss: 0.1263, Test Loss: 0.1155, L1: 0.0328, L0: 27.6419\n",
      "Epoch 80, Loss: 0.1233, Test Loss: 0.1129, L1: 0.0339, L0: 26.5443\n",
      "Epoch 90, Loss: 0.1213, Test Loss: 0.1106, L1: 0.0348, L0: 25.7292\n",
      "Epoch 100, Loss: 0.1201, Test Loss: 0.1098, L1: 0.0357, L0: 25.5664\n",
      "Epoch 110, Loss: 0.1184, Test Loss: 0.1112, L1: 0.0360, L0: 24.8607\n",
      "Epoch 120, Loss: 0.1173, Test Loss: 0.1106, L1: 0.0362, L0: 24.4492\n",
      "Epoch 130, Loss: 0.1169, Test Loss: 0.1085, L1: 0.0369, L0: 24.0521\n",
      "Epoch 140, Loss: 0.1161, Test Loss: 0.1091, L1: 0.0369, L0: 23.6406\n",
      "Epoch 150, Loss: 0.1153, Test Loss: 0.1083, L1: 0.0371, L0: 23.5312\n",
      "Epoch 160, Loss: 0.1144, Test Loss: 0.1064, L1: 0.0369, L0: 23.0195\n",
      "Epoch 170, Loss: 0.1142, Test Loss: 0.1061, L1: 0.0371, L0: 22.8880\n",
      "Epoch 180, Loss: 0.1135, Test Loss: 0.1022, L1: 0.0370, L0: 22.6732\n",
      "Epoch 190, Loss: 0.1135, Test Loss: 0.1062, L1: 0.0372, L0: 22.5000\n",
      "Epoch 200, Loss: 0.1126, Test Loss: 0.1089, L1: 0.0369, L0: 21.9948\n",
      "Epoch 210, Loss: 0.1125, Test Loss: 0.1088, L1: 0.0371, L0: 21.9883\n",
      "Epoch 220, Loss: 0.1124, Test Loss: 0.1087, L1: 0.0370, L0: 21.8294\n",
      "Epoch 230, Loss: 0.1118, Test Loss: 0.1072, L1: 0.0368, L0: 21.7279\n",
      "Epoch 240, Loss: 0.1117, Test Loss: 0.1084, L1: 0.0370, L0: 21.8177\n",
      "Epoch 250, Loss: 0.1112, Test Loss: 0.1061, L1: 0.0369, L0: 21.5378\n",
      "Epoch 260, Loss: 0.1110, Test Loss: 0.1095, L1: 0.0367, L0: 21.3594\n",
      "Epoch 270, Loss: 0.1104, Test Loss: 0.1087, L1: 0.0364, L0: 21.1745\n",
      "Epoch 280, Loss: 0.1104, Test Loss: 0.1059, L1: 0.0365, L0: 20.9401\n",
      "Epoch 290, Loss: 0.1104, Test Loss: 0.1067, L1: 0.0365, L0: 21.0417\n",
      "Epoch 300, Loss: 0.1099, Test Loss: 0.1068, L1: 0.0364, L0: 20.8789\n",
      "Epoch 310, Loss: 0.1103, Test Loss: 0.1075, L1: 0.0366, L0: 20.7552\n",
      "Epoch 320, Loss: 0.1099, Test Loss: 0.1059, L1: 0.0365, L0: 20.6484\n",
      "Epoch 330, Loss: 0.1099, Test Loss: 0.1046, L1: 0.0366, L0: 20.6562\n",
      "Epoch 340, Loss: 0.1096, Test Loss: 0.1061, L1: 0.0365, L0: 20.4440\n",
      "Epoch 350, Loss: 0.1090, Test Loss: 0.1085, L1: 0.0361, L0: 20.2070\n",
      "Epoch 360, Loss: 0.1092, Test Loss: 0.1082, L1: 0.0362, L0: 20.0729\n",
      "Epoch 370, Loss: 0.1096, Test Loss: 0.1068, L1: 0.0365, L0: 20.2305\n",
      "Epoch 380, Loss: 0.1088, Test Loss: 0.1071, L1: 0.0362, L0: 19.9766\n",
      "Epoch 390, Loss: 0.1090, Test Loss: 0.1072, L1: 0.0362, L0: 19.8581\n",
      "Epoch 400, Loss: 0.1089, Test Loss: 0.1082, L1: 0.0361, L0: 19.8073\n",
      "Epoch 410, Loss: 0.1087, Test Loss: 0.1079, L1: 0.0361, L0: 19.8659\n",
      "Epoch 420, Loss: 0.1086, Test Loss: 0.1031, L1: 0.0361, L0: 19.8307\n",
      "Epoch 430, Loss: 0.1085, Test Loss: 0.1101, L1: 0.0361, L0: 19.5964\n",
      "Epoch 440, Loss: 0.1080, Test Loss: 0.1079, L1: 0.0358, L0: 19.4453\n",
      "Epoch 450, Loss: 0.1082, Test Loss: 0.1101, L1: 0.0358, L0: 19.5117\n",
      "Epoch 460, Loss: 0.1083, Test Loss: 0.1067, L1: 0.0360, L0: 19.4349\n",
      "Epoch 470, Loss: 0.1079, Test Loss: 0.1061, L1: 0.0357, L0: 19.2214\n",
      "Epoch 480, Loss: 0.1082, Test Loss: 0.1071, L1: 0.0360, L0: 19.2747\n",
      "Epoch 490, Loss: 0.1081, Test Loss: 0.1088, L1: 0.0360, L0: 19.3659\n",
      "Epoch 500, Loss: 0.1082, Test Loss: 0.1065, L1: 0.0361, L0: 19.1263\n",
      "Epoch 510, Loss: 0.1079, Test Loss: 0.1083, L1: 0.0358, L0: 19.0495\n",
      "Epoch 520, Loss: 0.1078, Test Loss: 0.1084, L1: 0.0359, L0: 19.0534\n",
      "Epoch 530, Loss: 0.1078, Test Loss: 0.1059, L1: 0.0358, L0: 19.0482\n",
      "Epoch 540, Loss: 0.1077, Test Loss: 0.1092, L1: 0.0357, L0: 19.0599\n",
      "Epoch 550, Loss: 0.1079, Test Loss: 0.1084, L1: 0.0358, L0: 18.8698\n",
      "Epoch 560, Loss: 0.1080, Test Loss: 0.1078, L1: 0.0361, L0: 18.9154\n",
      "Epoch 570, Loss: 0.1076, Test Loss: 0.1071, L1: 0.0358, L0: 18.7214\n",
      "Epoch 580, Loss: 0.1078, Test Loss: 0.1053, L1: 0.0359, L0: 18.8516\n",
      "Epoch 590, Loss: 0.1074, Test Loss: 0.1058, L1: 0.0359, L0: 18.7760\n",
      "Epoch 600, Loss: 0.1073, Test Loss: 0.1048, L1: 0.0358, L0: 18.7292\n",
      "Epoch 610, Loss: 0.1075, Test Loss: 0.1058, L1: 0.0359, L0: 18.5859\n",
      "Epoch 620, Loss: 0.1072, Test Loss: 0.1063, L1: 0.0357, L0: 18.7539\n",
      "Epoch 630, Loss: 0.1076, Test Loss: 0.1074, L1: 0.0359, L0: 18.6888\n",
      "Epoch 640, Loss: 0.1075, Test Loss: 0.1057, L1: 0.0360, L0: 18.6602\n",
      "Epoch 650, Loss: 0.1076, Test Loss: 0.1072, L1: 0.0360, L0: 18.7747\n",
      "Epoch 660, Loss: 0.1072, Test Loss: 0.1071, L1: 0.0357, L0: 18.5312\n",
      "Epoch 670, Loss: 0.1074, Test Loss: 0.1096, L1: 0.0359, L0: 18.7266\n",
      "Epoch 680, Loss: 0.1075, Test Loss: 0.1053, L1: 0.0360, L0: 18.6354\n",
      "Epoch 690, Loss: 0.1072, Test Loss: 0.1078, L1: 0.0358, L0: 18.6979\n",
      "Epoch 700, Loss: 0.1073, Test Loss: 0.1065, L1: 0.0360, L0: 18.7422\n",
      "Epoch 710, Loss: 0.1075, Test Loss: 0.1098, L1: 0.0360, L0: 18.7539\n",
      "Epoch 720, Loss: 0.1074, Test Loss: 0.1079, L1: 0.0360, L0: 18.5938\n",
      "Epoch 730, Loss: 0.1071, Test Loss: 0.1065, L1: 0.0357, L0: 18.5990\n",
      "Epoch 740, Loss: 0.1072, Test Loss: 0.1055, L1: 0.0359, L0: 18.5768\n",
      "Epoch 750, Loss: 0.1072, Test Loss: 0.1082, L1: 0.0359, L0: 18.5365\n",
      "Epoch 760, Loss: 0.1072, Test Loss: 0.1063, L1: 0.0359, L0: 18.6732\n",
      "Epoch 770, Loss: 0.1069, Test Loss: 0.1073, L1: 0.0357, L0: 18.5430\n",
      "Epoch 780, Loss: 0.1072, Test Loss: 0.1085, L1: 0.0361, L0: 18.6380\n",
      "Epoch 790, Loss: 0.1071, Test Loss: 0.1054, L1: 0.0358, L0: 18.4518\n",
      "Epoch 800, Loss: 0.1070, Test Loss: 0.1071, L1: 0.0359, L0: 18.4857\n",
      "Epoch 810, Loss: 0.1069, Test Loss: 0.1067, L1: 0.0357, L0: 18.4883\n",
      "Epoch 820, Loss: 0.1068, Test Loss: 0.1048, L1: 0.0358, L0: 18.4466\n",
      "Epoch 830, Loss: 0.1070, Test Loss: 0.1064, L1: 0.0358, L0: 18.5091\n",
      "Epoch 840, Loss: 0.1070, Test Loss: 0.1052, L1: 0.0358, L0: 18.5065\n",
      "Epoch 850, Loss: 0.1071, Test Loss: 0.1050, L1: 0.0359, L0: 18.4479\n",
      "Epoch 860, Loss: 0.1070, Test Loss: 0.1067, L1: 0.0359, L0: 18.4883\n",
      "Epoch 870, Loss: 0.1068, Test Loss: 0.1065, L1: 0.0358, L0: 18.3815\n",
      "Epoch 880, Loss: 0.1069, Test Loss: 0.1069, L1: 0.0358, L0: 18.4818\n",
      "Epoch 890, Loss: 0.1070, Test Loss: 0.1051, L1: 0.0359, L0: 18.5469\n",
      "Epoch 900, Loss: 0.1069, Test Loss: 0.1059, L1: 0.0359, L0: 18.5091\n",
      "Epoch 910, Loss: 0.1069, Test Loss: 0.1055, L1: 0.0357, L0: 18.3034\n",
      "Epoch 920, Loss: 0.1069, Test Loss: 0.1094, L1: 0.0359, L0: 18.5547\n",
      "Epoch 930, Loss: 0.1068, Test Loss: 0.1063, L1: 0.0357, L0: 18.4753\n",
      "Epoch 940, Loss: 0.1069, Test Loss: 0.1068, L1: 0.0358, L0: 18.5182\n",
      "Epoch 950, Loss: 0.1067, Test Loss: 0.1066, L1: 0.0357, L0: 18.3633\n",
      "Epoch 960, Loss: 0.1069, Test Loss: 0.1098, L1: 0.0361, L0: 18.4740\n",
      "Epoch 970, Loss: 0.1067, Test Loss: 0.1045, L1: 0.0359, L0: 18.4596\n",
      "Epoch 980, Loss: 0.1067, Test Loss: 0.1074, L1: 0.0357, L0: 18.3776\n",
      "Epoch 990, Loss: 0.1066, Test Loss: 0.1066, L1: 0.0358, L0: 18.2617\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
